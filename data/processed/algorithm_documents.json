[
  {
    "content": "Algorithm: Quick Sort\nDescription: Quick sort is a highly efficient sorting algorithm that uses a divide-and-conquer strategy. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\nTags: sorting, divide and conquer, comparison sort\nComplexity (Time): O(n log n)\nComplexity (Space): O(log n)\nUse Cases:",
    "metadata": {
      "id": "62a41d19-471c-4370-943d-a1f335ab4531",
      "title": "",
      "name": "Quick Sort",
      "difficulty": "",
      "tags": [
        "sorting",
        "divide and conquer",
        "comparison sort"
      ]
    }
  },
  {
    "content": "Algorithm: Merge Sort\nDescription: Merge sort is an efficient, stable, comparison-based, divide and conquer sorting algorithm. It divides the input array into two halves, recursively sorts them, and then merges the sorted halves. The merge step is the key operation, where the two sorted sub-arrays are combined to form a single sorted array.\nTags: sorting, divide and conquer, stable sort\nComplexity (Time): O(n log n)\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "2e6e60af-42dd-43d6-ab07-ebece9734190",
      "title": "",
      "name": "Merge Sort",
      "difficulty": "",
      "tags": [
        "sorting",
        "divide and conquer",
        "stable sort"
      ]
    }
  },
  {
    "content": "Algorithm: Heap Sort\nDescription: Heap sort is a comparison-based sorting algorithm that uses a binary heap data structure. It divides its input into a sorted region and an unsorted region, and iteratively shrinks the unsorted region by extracting the largest element and inserting it into the sorted region.\nTags: sorting, comparison sort, in-place\nComplexity (Time): O(n log n)\nComplexity (Space): O(1)\nUse Cases:",
    "metadata": {
      "id": "880f43d7-96b1-45ca-9c81-996b4c5bc1f0",
      "title": "",
      "name": "Heap Sort",
      "difficulty": "",
      "tags": [
        "sorting",
        "comparison sort",
        "in-place"
      ]
    }
  },
  {
    "content": "Algorithm: Binary Search\nDescription: Binary search is an efficient algorithm for finding an item from a sorted list of items. It works by repeatedly dividing in half the portion of the list that could contain the item, until you've narrowed down the possible locations to just one.\nTags: searching, divide and conquer, sorted array\nComplexity (Time): O(log n)\nComplexity (Space): O(1)\nUse Cases:\nExample Problems:\n1. Search in a sorted array\n2. Find first and last occurrence of an element\n3. Search in a rotated sorted array\n4. Find peak element\n",
    "metadata": {
      "id": "26099e2c-d358-4d93-8b45-4cc4ac1516f2",
      "title": "",
      "name": "Binary Search",
      "difficulty": "",
      "tags": [
        "searching",
        "divide and conquer",
        "sorted array"
      ]
    }
  },
  {
    "content": "Algorithm: Depth-First Search (DFS)\nDescription: Depth-First Search is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node and explores as far as possible along each branch before backtracking.\nTags: searching, graph algorithm, tree traversal\nComplexity (Time): O(V + E)\nComplexity (Space): O(V)\nUse Cases:\nExample Problems:\n1. Shortest path problem\n2. Minimum spanning tree\n3. Detect cycle in a graph\n4. Topological sorting\n",
    "metadata": {
      "id": "c8deeb3b-d101-41ef-a4c1-d0ede9ed56fd",
      "title": "",
      "name": "Depth-First Search (DFS)",
      "difficulty": "",
      "tags": [
        "searching",
        "graph algorithm",
        "tree traversal"
      ]
    }
  },
  {
    "content": "Algorithm: Breadth-First Search (BFS)\nDescription: Breadth-First Search is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node in a graph) and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\nTags: searching, graph algorithm, tree traversal\nComplexity (Time): O(V + E)\nComplexity (Space): O(V)\nUse Cases:\nExample Problems:\n1. Shortest path problem\n2. Minimum spanning tree\n3. Detect cycle in a graph\n4. Topological sorting\n",
    "metadata": {
      "id": "902a354d-685c-45e3-97fe-5c29d7b09df1",
      "title": "",
      "name": "Breadth-First Search (BFS)",
      "difficulty": "",
      "tags": [
        "searching",
        "graph algorithm",
        "tree traversal"
      ]
    }
  },
  {
    "content": "Algorithm: Dijkstra's Algorithm\nDescription: Dijkstra's algorithm is used to find the shortest paths between nodes in a graph with non-negative edge weights. It uses a priority queue to greedily select the closest vertex that has not yet been processed and updates the distances to all its neighbors.\nTags: graph algorithm, shortest path, weighted graph, graph, shortest_path\nComplexity (Time): O((V + E) log V)\nComplexity (Space): O(V)\nUse Cases:\nExample Problems:\n1. Shortest path problem\n2. Minimum spanning tree\n3. Detect cycle in a graph\n4. Topological sorting\n",
    "metadata": {
      "id": "1f20bef5-784e-40bd-92a5-79fab9dbe6e7",
      "title": "",
      "name": "Dijkstra's Algorithm",
      "difficulty": "",
      "tags": [
        "graph algorithm",
        "shortest path",
        "weighted graph",
        "graph",
        "shortest_path"
      ]
    }
  },
  {
    "content": "Algorithm: Kruskal's Algorithm\nDescription: Kruskal's algorithm is a greedy algorithm that finds a minimum spanning tree for a connected weighted graph. It adds the edges in order of their weight (smallest to largest) as long as adding an edge doesn't create a cycle.\nTags: graph algorithm, minimum spanning tree, greedy\nComplexity (Time): O(E log E)\nComplexity (Space): O(V + E)\nUse Cases:\nExample Problems:\n1. Shortest path problem\n2. Minimum spanning tree\n3. Detect cycle in a graph\n4. Topological sorting\n",
    "metadata": {
      "id": "58a726c8-3afe-47a7-b6f2-fb2b50e462c6",
      "title": "",
      "name": "Kruskal's Algorithm",
      "difficulty": "",
      "tags": [
        "graph algorithm",
        "minimum spanning tree",
        "greedy"
      ]
    }
  },
  {
    "content": "Algorithm: Topological Sort\nDescription: Topological Sort is an algorithm for ordering the vertices of a directed acyclic graph (DAG) such that for every directed edge (u, v), vertex u comes before vertex v in the ordering.\nTags: graph algorithm, directed acyclic graph, ordering\nComplexity (Time): O(V + E)\nComplexity (Space): O(V)\nUse Cases:\nExample Problems:\n1. Shortest path problem\n2. Minimum spanning tree\n3. Detect cycle in a graph\n4. Topological sorting\n",
    "metadata": {
      "id": "17064309-b19e-42e8-aa53-fcb1b8160c44",
      "title": "",
      "name": "Topological Sort",
      "difficulty": "",
      "tags": [
        "graph algorithm",
        "directed acyclic graph",
        "ordering"
      ]
    }
  },
  {
    "content": "Algorithm: Linked List Reversal\nDescription: The Linked List Reversal algorithm takes a singly linked list and reverses the order of its nodes in-place by manipulating the pointers. This is done by iterating through the list and changing each node's next pointer to point to the previous node instead of the next one.\nTags: linked list, pointer manipulation, in-place, data_structures, linear\nComplexity (Time): O(n)\nComplexity (Space): O(1)\nUse Cases:",
    "metadata": {
      "id": "a415732d-3fbc-4acc-b9c8-261ed003a205",
      "title": "",
      "name": "Linked List Reversal",
      "difficulty": "",
      "tags": [
        "linked list",
        "pointer manipulation",
        "in-place",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Linked List Cycle Detection\nDescription: The Linked List Cycle Detection algorithm (also known as Floyd's Tortoise and Hare algorithm) determines if a linked list has a cycle by using two pointers that move at different speeds. If there is a cycle, the fast pointer will eventually catch up to the slow pointer.\nTags: linked list, two pointers, cycle detection, data_structures, linear\nComplexity (Time): O(n)\nComplexity (Space): O(1)\nUse Cases:\nExample Problems:\n1. Two sum problem\n2. Container with most water\n3. Remove duplicates from sorted array\n4. Finding triplets that sum to zero\n",
    "metadata": {
      "id": "8ee8179c-ba82-4a61-b819-e36c7efb55ff",
      "title": "",
      "name": "Linked List Cycle Detection",
      "difficulty": "",
      "tags": [
        "linked list",
        "two pointers",
        "cycle detection",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Linked List Rotation\nDescription: The Linked List Rotation algorithm rotates a linked list to the right or left by k positions by manipulating pointers. The operation is performed by connecting the tail of the list to the head to form a circle, then breaking the circle at the appropriate point.\nTags: linked list, pointer manipulation, two pointers, data_structures, linear\nComplexity (Time): O(n)\nComplexity (Space): O(1)\nUse Cases:\nExample Problems:\n1. Two sum problem\n2. Container with most water\n3. Remove duplicates from sorted array\n4. Finding triplets that sum to zero\n",
    "metadata": {
      "id": "35a1b5c8-fad6-4ed6-bd8e-8cb8757d5f2b",
      "title": "",
      "name": "Linked List Rotation",
      "difficulty": "",
      "tags": [
        "linked list",
        "pointer manipulation",
        "two pointers",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Linked List Merge\nDescription: The Linked List Merge algorithm combines two sorted linked lists into a single sorted linked list by comparing nodes from both lists and linking them in the correct order.\nTags: linked list, two pointers, sorting\nComplexity (Time): O(n + m)\nComplexity (Space): O(1)\nUse Cases:\nExample Problems:\n1. Two sum problem\n2. Container with most water\n3. Remove duplicates from sorted array\n4. Finding triplets that sum to zero\n",
    "metadata": {
      "id": "4075164e-26ec-4b4d-a0fd-8707080dce41",
      "title": "",
      "name": "Linked List Merge",
      "difficulty": "",
      "tags": [
        "linked list",
        "two pointers",
        "sorting"
      ]
    }
  },
  {
    "content": "Algorithm: Stack Implementation\nDescription: The Stack data structure follows Last-In-First-Out (LIFO) principle. It supports two primary operations: push (adding an element to the top) and pop (removing the top element). Stacks can be implemented using arrays or linked lists.\nTags: stack, data structure, LIFO, data_structures, linear\nComplexity (Time): O(1) for push/pop operations\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "12b329e9-5e2d-40f0-a5ec-6f47c621cc20",
      "title": "",
      "name": "Stack Implementation",
      "difficulty": "",
      "tags": [
        "stack",
        "data structure",
        "LIFO",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Balanced Parentheses Check\nDescription: The Balanced Parentheses Check algorithm uses a stack to verify if an expression has balanced parentheses, brackets, and braces. It scans the expression from left to right, pushing opening delimiters onto a stack and popping when matching closing delimiters are encountered.\nTags: stack, string, validation\nComplexity (Time): O(n)\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "6c8c478b-db64-47c8-af04-e46d47c4f7b5",
      "title": "",
      "name": "Balanced Parentheses Check",
      "difficulty": "",
      "tags": [
        "stack",
        "string",
        "validation"
      ]
    }
  },
  {
    "content": "Algorithm: Infix to Postfix Conversion\nDescription: The Infix to Postfix Conversion algorithm transforms an infix expression (standard mathematical notation with operators between operands) to postfix notation (operators follow their operands) using a stack to handle operator precedence and parentheses.\nTags: stack, expression, conversion\nComplexity (Time): O(n)\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "24190139-9938-48e6-bf75-6b4bca8f6269",
      "title": "",
      "name": "Infix to Postfix Conversion",
      "difficulty": "",
      "tags": [
        "stack",
        "expression",
        "conversion"
      ]
    }
  },
  {
    "content": "Algorithm: 0/1 Knapsack\nDescription: The 0/1 Knapsack problem is a problem in combinatorial optimization: given a set of items, each with a weight and a value, determine which items to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.\nTags: dynamic programming, optimization, combinatorial, dynamic_programming, optimization\nComplexity (Time): O(n*W)\nComplexity (Space): O(n*W)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "60c4ee04-911c-46a0-b202-8d9d8b212a61",
      "title": "",
      "name": "0/1 Knapsack",
      "difficulty": "",
      "tags": [
        "dynamic programming",
        "optimization",
        "combinatorial",
        "dynamic_programming",
        "optimization"
      ]
    }
  },
  {
    "content": "Algorithm: Longest Common Subsequence\nDescription: The Longest Common Subsequence (LCS) algorithm finds the longest sequence that is present in both given sequences in the same order (not necessarily consecutive).\nTags: dynamic programming, string algorithm, sequence comparison, dynamic_programming, optimization\nComplexity (Time): O(m*n)\nComplexity (Space): O(m*n)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "e7e7406e-488e-4bcb-81bf-d7239b028068",
      "title": "",
      "name": "Longest Common Subsequence",
      "difficulty": "",
      "tags": [
        "dynamic programming",
        "string algorithm",
        "sequence comparison",
        "dynamic_programming",
        "optimization"
      ]
    }
  },
  {
    "content": "Algorithm: Coin Change\nDescription: The Coin Change problem asks for the minimum number of coins needed to make a certain amount of change, given a set of coin denominations.\nTags: dynamic programming, greedy, optimization\nComplexity (Time): O(amount * n)\nComplexity (Space): O(amount)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "ab7b5b41-6bab-421b-847d-048a624ac66e",
      "title": "",
      "name": "Coin Change",
      "difficulty": "",
      "tags": [
        "dynamic programming",
        "greedy",
        "optimization"
      ]
    }
  },
  {
    "content": "Algorithm: Knuth-Morris-Pratt (KMP)\nDescription: The Knuth-Morris-Pratt algorithm searches for occurrences of a 'pattern' within a main 'text' by employing the observation that when a mismatch occurs, the pattern itself contains sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.\nTags: string algorithm, pattern matching, substring search, string, pattern_matching\nComplexity (Time): O(n + m)\nComplexity (Space): O(m)\nUse Cases:",
    "metadata": {
      "id": "65cf2b0a-34ef-4985-810e-24693924b986",
      "title": "",
      "name": "Knuth-Morris-Pratt (KMP)",
      "difficulty": "",
      "tags": [
        "string algorithm",
        "pattern matching",
        "substring search",
        "string",
        "pattern_matching"
      ]
    }
  },
  {
    "content": "Algorithm: Rabin-Karp\nDescription: The Rabin-Karp algorithm is a string-searching algorithm that uses hashing to find patterns in strings. It calculates a hash value for the pattern and for each possible substring of the text, then compares the hash values instead of comparing the strings character by character.\nTags: string algorithm, pattern matching, hashing, string, pattern_matching\nComplexity (Time): O(n + m)\nComplexity (Space): O(1)\nUse Cases:",
    "metadata": {
      "id": "0e9f7833-9d18-427a-80a6-642c6d28f0a3",
      "title": "",
      "name": "Rabin-Karp",
      "difficulty": "",
      "tags": [
        "string algorithm",
        "pattern matching",
        "hashing",
        "string",
        "pattern_matching"
      ]
    }
  },
  {
    "content": "Algorithm: Longest Palindromic Substring\nDescription: The Longest Palindromic Substring algorithm finds the longest substring within a string that is a palindrome (reads the same backward as forward).\nTags: string algorithm, dynamic programming\nComplexity (Time): O(n\u00b2)\nComplexity (Space): O(1)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "7ba7938e-2c8d-4181-9217-46d583975a8e",
      "title": "",
      "name": "Longest Palindromic Substring",
      "difficulty": "",
      "tags": [
        "string algorithm",
        "dynamic programming"
      ]
    }
  },
  {
    "content": "Algorithm: Binary Tree Traversal\nDescription: Binary Tree Traversal algorithms systematically visit each node in a binary tree. The three most common traversal methods are in-order (left-root-right), pre-order (root-left-right), and post-order (left-right-root).\nTags: tree algorithm, data structure, traversal, data_structures, tree\nComplexity (Time): O(n)\nComplexity (Space): O(h)\nUse Cases:\nExample Problems:\n1. Inorder, preorder, postorder traversal\n2. Level order traversal\n3. Find height/depth of a tree\n4. Check if a binary tree is balanced\n",
    "metadata": {
      "id": "9b989cd7-6e10-4cc5-9647-cf5b1a68a39b",
      "title": "",
      "name": "Binary Tree Traversal",
      "difficulty": "",
      "tags": [
        "tree algorithm",
        "data structure",
        "traversal",
        "data_structures",
        "tree"
      ]
    }
  },
  {
    "content": "Algorithm: Binary Search Tree Operations\nDescription: Binary Search Tree (BST) operations include insertion, deletion, and searching in a tree where for each node, all elements in the left subtree are less than the node's value, and all elements in the right subtree are greater.\nTags: tree algorithm, binary search tree, data structure, searching, array_search\nComplexity (Time): O(h)\nComplexity (Space): O(h)\nUse Cases:\nExample Problems:\n1. Search in a sorted array\n2. Find first and last occurrence of an element\n3. Search in a rotated sorted array\n4. Find peak element\n",
    "metadata": {
      "id": "f57cfa89-5170-443f-b236-98f3e497573e",
      "title": "",
      "name": "Binary Search Tree Operations",
      "difficulty": "",
      "tags": [
        "tree algorithm",
        "binary search tree",
        "data structure",
        "searching",
        "array_search"
      ]
    }
  },
  {
    "content": "Algorithm: Lowest Common Ancestor\nDescription: The Lowest Common Ancestor (LCA) algorithm finds the lowest node in a tree that has both given nodes as descendants. A node can be a descendant of itself.\nTags: tree algorithm, binary tree, ancestor finding\nComplexity (Time): O(n)\nComplexity (Space): O(h)\nUse Cases:",
    "metadata": {
      "id": "2d698322-5474-4898-b0b4-d85ec99205ec",
      "title": "",
      "name": "Lowest Common Ancestor",
      "difficulty": "",
      "tags": [
        "tree algorithm",
        "binary tree",
        "ancestor finding"
      ]
    }
  },
  {
    "content": "Algorithm: Queue Implementation\nDescription: The Queue data structure follows First-In-First-Out (FIFO) principle. It supports two primary operations: enqueue (adding an element to the rear) and dequeue (removing the front element). Queues can be implemented using arrays, linked lists, or a combination of stacks.\nTags: queue, data structure, FIFO, data_structures, linear\nComplexity (Time): O(1) for enqueue/dequeue operations\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "0865fba0-b937-454f-ac20-0d09211ef903",
      "title": "",
      "name": "Queue Implementation",
      "difficulty": "",
      "tags": [
        "queue",
        "data structure",
        "FIFO",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Circular Queue Implementation\nDescription: A Circular Queue (also called Ring Buffer) is an enhancement of the regular queue that efficiently uses space by wrapping around to the beginning when it reaches the end of the allocated space. It maintains two pointers: front and rear, and uses modulo arithmetic to handle the wrap-around.\nTags: queue, circular, data structure, data_structures, linear\nComplexity (Time): O(1) for enqueue/dequeue operations\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "02a6b88b-9cc1-4314-8d04-76d2fac92261",
      "title": "",
      "name": "Circular Queue Implementation",
      "difficulty": "",
      "tags": [
        "queue",
        "circular",
        "data structure",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Priority Queue Implementation\nDescription: A Priority Queue is an abstract data type similar to a regular queue but where each element has a priority. Elements with higher priority are dequeued before elements with lower priority. It can be implemented using a heap, a binary search tree, or an ordered array.\nTags: queue, priority, heap, data structure, data_structures, linear\nComplexity (Time): O(log n) for insertion/deletion with heap implementation\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "084e8821-5c9d-4c01-9c67-1254dd1687b6",
      "title": "",
      "name": "Priority Queue Implementation",
      "difficulty": "",
      "tags": [
        "queue",
        "priority",
        "heap",
        "data structure",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Hash Table Implementation\nDescription: A Hash Table is a data structure that implements an associative array abstract data type, a structure that can map keys to values. It uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.\nTags: hash table, data structure, key-value, data_structures, hash\nComplexity (Time): O(1) average for insert/search/delete, O(n) worst case\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "d129490c-6a73-4bf2-a316-6cfecf1f71ea",
      "title": "",
      "name": "Hash Table Implementation",
      "difficulty": "",
      "tags": [
        "hash table",
        "data structure",
        "key-value",
        "data_structures",
        "hash"
      ]
    }
  },
  {
    "content": "Algorithm: Collision Resolution with Chaining\nDescription: Collision Resolution with Chaining is a technique used in hash tables to handle multiple keys that hash to the same index. In chaining, each bucket (array index) contains a linked list of all key-value pairs whose keys hash to that index, allowing multiple entries to exist at the same location.\nTags: hash table, collision resolution, linked list\nComplexity (Time): O(1 + \u03b1) average for operations, where \u03b1 is the load factor\nComplexity (Space): O(n + m) where n is the number of entries and m is the number of buckets\nUse Cases:",
    "metadata": {
      "id": "3c208b99-14f1-47b6-8ab4-aceba745726b",
      "title": "",
      "name": "Collision Resolution with Chaining",
      "difficulty": "",
      "tags": [
        "hash table",
        "collision resolution",
        "linked list"
      ]
    }
  },
  {
    "content": "Algorithm: Open Addressing (Linear Probing)\nDescription: Open Addressing is a collision resolution technique where all elements are stored in the hash table itself (no external data structures). Linear Probing is one method of open addressing where, if a collision occurs, we sequentially search for the next available slot.\nTags: hash table, collision resolution, open addressing\nComplexity (Time): O(1) average for operations with low load factor, O(n) worst case\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "de9ef811-4174-477b-9e78-c529b67b5379",
      "title": "",
      "name": "Open Addressing (Linear Probing)",
      "difficulty": "",
      "tags": [
        "hash table",
        "collision resolution",
        "open addressing"
      ]
    }
  },
  {
    "content": "Algorithm: Trie (Prefix Tree)\nDescription: A Trie is a tree-like data structure used to store a dynamic set of strings. Tries are efficient for prefix-based operations and are commonly used for fast retrieval of keys in a dataset of strings. Unlike a binary search tree, no node in the trie stores the key associated with that node; instead, its position in the tree defines the key with which it is associated.\nTags: data structure, tree, string, prefix, search\nComplexity (Time): O(m) for insert/search/delete where m is the length of the key\nComplexity (Space): O(n * m) where n is the number of keys and m is the key length\nUse Cases:",
    "metadata": {
      "id": "1ecf38ea-6967-4c50-b65b-ab8ad498db43",
      "title": "",
      "name": "Trie (Prefix Tree)",
      "difficulty": "",
      "tags": [
        "data structure",
        "tree",
        "string",
        "prefix",
        "search"
      ]
    }
  },
  {
    "content": "Algorithm: Segment Tree\nDescription: A Segment Tree is a tree data structure for storing intervals or segments. It allows querying which of the stored segments contain a given point. It is, in principle, a static structure; that is, it's a structure that cannot be modified once it's built. A segment tree for a set of n intervals uses O(n log n) storage and can be built in O(n log n) time. Segment trees support range queries and updates in O(log n) time.\nTags: data structure, tree, range queries, interval tree\nComplexity (Time): O(n log n) to build, O(log n) for range query and update\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "946e0960-6e45-45e1-8182-fe26e5ceb203",
      "title": "",
      "name": "Segment Tree",
      "difficulty": "",
      "tags": [
        "data structure",
        "tree",
        "range queries",
        "interval tree"
      ]
    }
  },
  {
    "content": "Algorithm: Union Find (Disjoint Set)\nDescription: Union Find is a data structure that tracks a set of elements partitioned into a number of disjoint (non-overlapping) subsets. It provides near-constant-time operations to add new sets, merge existing sets, and determine whether elements are in the same set. Union Find is particularly useful for Kruskal's algorithm and for tracking connected components in graphs.\nTags: data structure, graph, disjoint set, connectivity\nComplexity (Time): O(\u03b1(n)) for find and union operations, where \u03b1(n) is the inverse Ackermann function\nComplexity (Space): O(n)\nUse Cases:\nExample Problems:\n1. Shortest path problem\n2. Minimum spanning tree\n3. Detect cycle in a graph\n4. Topological sorting\n",
    "metadata": {
      "id": "46f66bfa-74fb-4951-bc95-db0e3df9c587",
      "title": "",
      "name": "Union Find (Disjoint Set)",
      "difficulty": "",
      "tags": [
        "data structure",
        "graph",
        "disjoint set",
        "connectivity"
      ]
    }
  },
  {
    "content": "Algorithm: A* Search Algorithm\nDescription: A* (pronounced 'A star') is a pathfinding algorithm that finds the shortest path between two nodes. It uses a heuristic function to guide the search, making it more efficient than Dijkstra's algorithm in many cases. A* evaluates nodes by combining the cost to reach the node and the estimated cost to reach the goal. It's widely used in games, robotics, and navigation systems.\nTags: graph algorithm, pathfinding, heuristic, search, searching, graph_search\nComplexity (Time): O(E) in the worst case, where E is the number of edges, but typically much better with a good heuristic\nComplexity (Space): O(V), where V is the number of vertices\nUse Cases:\nExample Problems:\n1. Shortest path problem\n2. Minimum spanning tree\n3. Detect cycle in a graph\n4. Topological sorting\n",
    "metadata": {
      "id": "59a7f44f-200c-4b10-80bc-6381da2e275c",
      "title": "",
      "name": "A* Search Algorithm",
      "difficulty": "",
      "tags": [
        "graph algorithm",
        "pathfinding",
        "heuristic",
        "search",
        "searching",
        "graph_search"
      ]
    }
  },
  {
    "content": "Algorithm: Bellman-Ford Algorithm\nDescription: The Bellman-Ford algorithm finds the shortest paths from a single source vertex to all other vertices in a weighted graph. Unlike Dijkstra's algorithm, Bellman-Ford can handle graphs with negative weight edges. It also detects negative weight cycles, which are cycles whose edges sum to a negative value. The algorithm relaxes all edges V-1 times, where V is the number of vertices.\nTags: graph algorithm, shortest path, negative weight, dynamic programming, graph, shortest_path\nComplexity (Time): O(V * E) where V is the number of vertices and E is the number of edges\nComplexity (Space): O(V)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "00214f12-41bd-4dbe-aea1-5e69a07123bf",
      "title": "",
      "name": "Bellman-Ford Algorithm",
      "difficulty": "",
      "tags": [
        "graph algorithm",
        "shortest path",
        "negative weight",
        "dynamic programming",
        "graph",
        "shortest_path"
      ]
    }
  },
  {
    "content": "Algorithm: Floyd-Warshall Algorithm\nDescription: The Floyd-Warshall algorithm finds the shortest paths between all pairs of vertices in a weighted graph. It works with positive or negative edge weights and can detect negative cycles. The algorithm uses dynamic programming to gradually improve an estimate on the shortest path between two vertices by including intermediate vertices.\nTags: graph algorithm, all-pairs shortest path, dynamic programming, graph, shortest_path\nComplexity (Time): O(V\u00b3) where V is the number of vertices\nComplexity (Space): O(V\u00b2)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "73bccc92-f1fa-477d-965c-4d720988095d",
      "title": "",
      "name": "Floyd-Warshall Algorithm",
      "difficulty": "",
      "tags": [
        "graph algorithm",
        "all-pairs shortest path",
        "dynamic programming",
        "graph",
        "shortest_path"
      ]
    }
  },
  {
    "content": "Algorithm: Two Pointers Technique\nDescription: The Two Pointers technique is an algorithmic approach that uses two pointers to iterate through a data structure (usually an array or linked list). The pointers can move toward each other, in the same direction at different speeds, or start at different positions. This technique is often used to search for pairs, triplets, or subarrays with certain properties, and it typically reduces the time complexity from O(n\u00b2) to O(n).\nTags: algorithm technique, array, string, optimization\nComplexity (Time): O(n) in most cases\nComplexity (Space): O(1)\nUse Cases:\nExample Problems:\n1. Two sum problem\n2. Container with most water\n3. Remove duplicates from sorted array\n4. Finding triplets that sum to zero\n",
    "metadata": {
      "id": "ab43bd71-8da3-4459-9b72-7576706630cd",
      "title": "",
      "name": "Two Pointers Technique",
      "difficulty": "",
      "tags": [
        "algorithm technique",
        "array",
        "string",
        "optimization"
      ]
    }
  },
  {
    "content": "Algorithm: Monotonic Stack/Queue\nDescription: A Monotonic Stack is a stack that maintains its elements in either strictly increasing or strictly decreasing order. Similarly, a Monotonic Queue maintains the same property. These structures are particularly useful for problems involving finding the next greater/smaller element or maximum/minimum in a sliding window. By maintaining the monotonic property, these structures allow for efficient solving of these problems in linear time.\nTags: data structure, stack, queue, optimization, data_structures, linear\nComplexity (Time): O(n) for most operations (each element is pushed and popped at most once)\nComplexity (Space): O(n)\nUse Cases:",
    "metadata": {
      "id": "9e110b95-adb8-416d-8569-5c10b8f80cfa",
      "title": "",
      "name": "Monotonic Stack/Queue",
      "difficulty": "",
      "tags": [
        "data structure",
        "stack",
        "queue",
        "optimization",
        "data_structures",
        "linear"
      ]
    }
  },
  {
    "content": "Algorithm: Backtracking\nDescription: Backtracking is an algorithmic technique for solving problems recursively by trying to build a solution incrementally, one step at a time, removing solutions that fail to satisfy the constraints of the problem. It's like a systematic trial and error approach. The name comes from the fact that when you reach a state where you can't proceed further, you 'backtrack' to the previous state and try a different path.\nTags: algorithm technique, recursion, combinatorial, search\nComplexity (Time): O(b^d) where b is the branching factor and d is the maximum depth\nComplexity (Space): O(d) for the recursion stack\nUse Cases:",
    "metadata": {
      "id": "5ccaaba5-b7ef-4079-8aca-2df3d9a3b824",
      "title": "",
      "name": "Backtracking",
      "difficulty": "",
      "tags": [
        "algorithm technique",
        "recursion",
        "combinatorial",
        "search"
      ]
    }
  },
  {
    "content": "Algorithm: 0/1 Knapsack Pattern\nDescription: The 0/1 Knapsack pattern is a dynamic programming approach for problems where you have a set of items with values and weights, and you need to determine which items to include to maximize the value while staying within a weight constraint. Each item can either be included (1) or excluded (0), hence the name 0/1 Knapsack. This pattern extends to many problems involving decision-making with constraints.\nTags: dynamic programming, optimization, knapsack, decision, dynamic_programming, optimization\nComplexity (Time): O(n*W) where n is the number of items and W is the weight constraint\nComplexity (Space): O(n*W) for the standard approach, can be optimized to O(W)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "381043ed-3eaf-46c3-9f88-a0656cae9f07",
      "title": "",
      "name": "0/1 Knapsack Pattern",
      "difficulty": "",
      "tags": [
        "dynamic programming",
        "optimization",
        "knapsack",
        "decision",
        "dynamic_programming",
        "optimization"
      ]
    }
  },
  {
    "content": "Algorithm: Longest Common Subsequence Pattern\nDescription: The Longest Common Subsequence (LCS) pattern is a dynamic programming approach for finding the longest subsequence common to two sequences. A subsequence is a sequence that can be derived from another sequence by deleting some or no elements without changing the order of the remaining elements. The LCS pattern extends to many string and sequence comparison problems.\nTags: dynamic programming, string, sequence, comparison\nComplexity (Time): O(m*n) where m and n are the lengths of the sequences\nComplexity (Space): O(m*n)\nUse Cases:\nExample Problems:\n1. Longest common subsequence\n2. Knapsack problem\n3. Coin change problem\n4. Edit distance\n",
    "metadata": {
      "id": "9740ffb9-5226-4b8a-81ce-2cf6bed61f87",
      "title": "",
      "name": "Longest Common Subsequence Pattern",
      "difficulty": "",
      "tags": [
        "dynamic programming",
        "string",
        "sequence",
        "comparison"
      ]
    }
  }
]